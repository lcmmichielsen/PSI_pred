{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6945b4d2",
   "metadata": {},
   "source": [
    "# Training LR models\n",
    "\n",
    "In this notebook, we show how to train the LR models, make predictions and store the weights (for model interpretation). How to analyze the results, can be found in the figure repoducibility notebooks. \n",
    "\n",
    "Input for the functions can be downloaded from Zenodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe30c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as tm\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0480b74e",
   "metadata": {},
   "source": [
    "### LR model\n",
    "\n",
    "First, we create a pytorch lightning class for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86040641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticLinear(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, loss_fn, \n",
    "                 alpha=0.1, l1_ratio=0.5, lr=0.05):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters(ignore=['loss_fn'])\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.train_log = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "    def criterion(self, y_pred, y):\n",
    "        l1_loss = self.l1_ratio * torch.norm(self.linear.weight, p=1)\n",
    "        l2_loss = (1-self.l1_ratio) * torch.norm(self.linear.weight, p=2)\n",
    "        loss = self.loss_fn(y_pred, y) + self.alpha*(l1_loss+l2_loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, ind = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        \n",
    "        self.log(\"loss\", loss, on_epoch=True)\n",
    "        self.train_log.append(loss.detach().numpy())\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y, ind = batch\n",
    "        return self(x), y\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, ind = batch\n",
    "        y_pred = self(x)\n",
    "        test_loss = self.criterion(y_pred, y)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, ind = batch\n",
    "        y_pred = self(x)\n",
    "        val_loss = self.criterion(y_pred, y)\n",
    "        self.log(\"val_loss\", val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3eebf7",
   "metadata": {},
   "source": [
    "### Create a data module\n",
    "\n",
    "Create a data module that uses the RBP data as input and PSI values as output. We ensure that the folds are the same as for the DL models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96acf019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExonDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 PSI_neur,\n",
    "                 PSI_glia,\n",
    "                 RBP,\n",
    "                 out_dir_neur: str = 'results_test/neurons/modelxx/foldxx/runxx',\n",
    "                 out_dir_glia: str = 'results_test/glia/modelxx/foldxx/runxx',\n",
    "                 batch_size: int = 32,\n",
    "                 var_thres: float = 0.0,\n",
    "                 fold: int = 0,\n",
    "                celltype: int = -1):\n",
    "        super().__init__()\n",
    "        self.PSI_neur = PSI_neur\n",
    "        self.PSI_glia = PSI_glia\n",
    "        self.RBP = RBP\n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "        self.out_dir_neur = out_dir_neur\n",
    "        self.out_dir_glia = out_dir_glia\n",
    "        self.var_thres = var_thres\n",
    "        self.celltype = celltype\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \n",
    "        Path(self.out_dir_neur).mkdir(parents=True, exist_ok=True)\n",
    "        Path(self.out_dir_glia).mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "        # Only keep exons which have a PSI value for both glia and neurons\n",
    "        PSI = pd.concat((PSI_neur, PSI_glia), axis=1)\n",
    "        \n",
    "        split = np.array(pd.DataFrame(PSI_neur.index)[0].str.split('_', expand=True, n=5))\n",
    "        genes_all = split[:,3]\n",
    "                \n",
    "        self.PSI = PSI\n",
    "        self.RBP = RBP        \n",
    "        \n",
    "        # Split in 10 folds and pick fold of interest\n",
    "        gkf = GroupKFold(n_splits=10)\n",
    "        trainval_test_indices = list(gkf.split(self.RBP, self.PSI, genes_all))\n",
    "        trainval_indices, test_idx = trainval_test_indices[self.fold]\n",
    "        \n",
    "        gkf2 = GroupKFold(n_splits=9)\n",
    "        train_val_indices = list(gkf2.split(self.RBP.iloc[trainval_indices],\n",
    "                                          self.PSI.iloc[trainval_indices],\n",
    "                                          genes_all[trainval_indices]))\n",
    "        train_indices, val_indices = train_val_indices[0]\n",
    "        train_idx = trainval_indices[train_indices]\n",
    "        val_idx = trainval_indices[val_indices]\n",
    "        \n",
    "        # Only keep exons with at least some RBP counts\n",
    "        idx_tokeep =  np.where(np.sum(RBP, axis=1) > 0)[0]\n",
    "        train_idx = np.intersect1d(train_idx, idx_tokeep)\n",
    "        val_idx = np.intersect1d(val_idx, idx_tokeep)\n",
    "        test_idx = np.intersect1d(test_idx, idx_tokeep)\n",
    "        \n",
    "        # Filter for variable exons if needed\n",
    "        if self.var_thres > 0:\n",
    "            tokeep = np.where(np.abs(PSI.values[:,0] - PSI.values[:,1]) > self.var_thres)[0]\n",
    "            self.train_idx = np.intersect1d(train_idx, tokeep)\n",
    "            self.val_idx = np.intersect1d(val_idx, tokeep)\n",
    "        else:\n",
    "            self.train_idx = train_idx\n",
    "            self.val_idx = val_idx\n",
    "        \n",
    "        self.test_idx = test_idx\n",
    "        \n",
    "        print(len(self.train_idx))\n",
    "        print(len(self.val_idx))\n",
    "        print(len(self.test_idx))\n",
    "        \n",
    "        # Write the file genes.csv, so we know which genes were in which fold\n",
    "        split = np.zeros((len(self.RBP),1), dtype='<U5')\n",
    "        split[self.train_idx] = 'train'\n",
    "        split[self.val_idx] = 'valid'\n",
    "        split[self.test_idx] = 'test'\n",
    "        \n",
    "        #tokeep \n",
    "        target = np.reshape(PSI.values[:,0], (-1,1))\n",
    "        genes = pd.DataFrame(np.hstack((split,target)), \n",
    "                             index=PSI.index, columns=np.hstack((['split'], PSI.columns[0])))\n",
    "        genes = genes[genes['split'] != '']\n",
    "        genes.to_csv(self.out_dir_neur + '/genes.csv')\n",
    "        \n",
    "        target = np.reshape(PSI.values[:,1], (-1,1))\n",
    "        genes = pd.DataFrame(np.hstack((split,target)), \n",
    "                             index=PSI.index, columns=np.hstack((['split'], PSI.columns[1])))\n",
    "        genes = genes[genes['split'] != '']\n",
    "        genes.to_csv(self.out_dir_glia + '/genes.csv')\n",
    "\n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        \n",
    "        if stage == \"fit\":\n",
    "            self.RBP_train = RBP_data(self.PSI, self.RBP, self.train_idx, self.celltype)\n",
    "            self.RBP_val = RBP_data(self.PSI, self.RBP, self.val_idx, self.celltype)\n",
    "            \n",
    "        if stage == \"test\":\n",
    "            self.RBP_test = RBP_data(self.PSI, self.RBP, self.test_idx, self.celltype)\n",
    "            \n",
    "        if stage == \"predict\":\n",
    "            self.RBP_predict = RBP_data(self.PSI, self.RBP, self.test_idx, self.celltype)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.RBP_train, batch_size = self.batch_size,\n",
    "                          num_workers = 1,\n",
    "                         shuffle = True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.RBP_val, batch_size = self.batch_size,\n",
    "                          num_workers = 1,\n",
    "                         shuffle = False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.RBP_test, batch_size = self.batch_size,\n",
    "                          num_workers = 1,\n",
    "                         shuffle = False)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.RBP_predict, batch_size = self.batch_size,\n",
    "                          num_workers = 1,\n",
    "                         shuffle = False)\n",
    "    \n",
    "    \n",
    "class RBP_data(Dataset):\n",
    "    def __init__(self, PSI, RBP, idx, celltype):\n",
    "        \n",
    "        if celltype != -1:\n",
    "            self.PSI = np.reshape(PSI.values[idx,celltype], (-1,1))\n",
    "        else:\n",
    "            self.PSI = PSI.values[idx]\n",
    "        self.RBP = RBP.values[idx]\n",
    "        self.indices = idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.PSI)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        PSI = self.PSI[index].astype(np.float32)\n",
    "        RBP = self.RBP[index].astype(np.float32)\n",
    "        ind = self.indices[index].astype(int)\n",
    "        \n",
    "        sample = [RBP, PSI, ind]\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846235f",
   "metadata": {},
   "source": [
    "### Function to do the cross-validation\n",
    "\n",
    "This function trains the model, makes predictions, and save the weights learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df57671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_celltype(PSI_neur, PSI_glia, RBP, lr, alpha, l1_ratio, batch_size, var_thres,\n",
    "                      out_neurons, out_glia, celltype):\n",
    "    # Celltype = 0 for neurons\n",
    "    # Celltype = 1 for glia\n",
    "    \n",
    "    # Keeps track of the learned weights\n",
    "    weights = 0\n",
    "    \n",
    "    # 10 fold CV\n",
    "    for i in range(10):\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        # 5 runs per fold\n",
    "        for j in range(5):\n",
    "\n",
    "            out_neur2 = out_neurons + '/fold' + str(i) + '/run' + str(j)\n",
    "            out_glia2 = out_glia + '/fold' + str(i) + '/run' + str(j)\n",
    "\n",
    "            # Initialize data module\n",
    "            dm = ExonDataModule(PSI_neur=PSI_neur,\n",
    "                                PSI_glia=PSI_glia,\n",
    "                                RBP=RBP,\n",
    "                                batch_size=batch_size, fold=i,\n",
    "                                out_dir_neur=out_neur2,\n",
    "                                out_dir_glia=out_glia2,\n",
    "                                var_thres=var_thres,\n",
    "                                celltype=celltype)\n",
    "            \n",
    "            # Initialize model\n",
    "            model = ElasticLinear(input_dim = 732,\n",
    "                                  output_dim = 1,\n",
    "                                  loss_fn = torch.nn.BCEWithLogitsLoss(),\n",
    "                                  alpha=alpha, l1_ratio=l1_ratio, lr=lr)\n",
    "            early_stop_callback = EarlyStopping(monitor=\"val_loss\", \n",
    "                                                min_delta=0.00, \n",
    "                                                patience=10, verbose=False, mode=\"min\")\n",
    "            trainer = pl.Trainer(callbacks=[early_stop_callback], \n",
    "                                 max_epochs=-1,gpus=1)\n",
    "\n",
    "            # Train model\n",
    "            trainer.fit(model, datamodule=dm)\n",
    "\n",
    "            # Save the weights\n",
    "            weights += model.linear.weight.detach().numpy()/50\n",
    "\n",
    "            # Make predictions on test set\n",
    "            y_pred_true = trainer.predict(model, datamodule=dm)\n",
    "\n",
    "            # Extract predictions\n",
    "            y_pred_hip = []\n",
    "            y_true_hip = []\n",
    "\n",
    "            for j in range(len(y_pred_true)):\n",
    "\n",
    "                y_pred_hip.extend(y_pred_true[j][0].detach().numpy())\n",
    "                y_true_hip.extend(y_pred_true[j][1].detach().numpy())\n",
    "\n",
    "            y_pred_hip = np.array(y_pred_hip)\n",
    "            y_true_hip = np.array(y_true_hip)\n",
    "\n",
    "            # Sigmoid activation function\n",
    "            y_pred_hip = 1/(1 + np.exp(-y_pred_hip))\n",
    "            \n",
    "            if celltype == 0:\n",
    "                out_dir = out_neur2\n",
    "            else:\n",
    "                out_dir = out_glia2\n",
    "\n",
    "            # Save the results\n",
    "            genes_df = pd.read_csv(out_dir + '/genes.csv', index_col=0)\n",
    "            gene_ids = np.array(genes_df.index[genes_df['split'] == 'test'], dtype='S')\n",
    "\n",
    "            preds_h5 = h5py.File('%s/preds.h5' % out_dir, 'w')\n",
    "            preds_h5.create_dataset('preds', data=y_pred_hip)\n",
    "            preds_h5.create_dataset('genes', data=gene_ids)\n",
    "            preds_h5.close()\n",
    "\n",
    "            targets_h5 = h5py.File('%s/targets.h5' % out_dir, 'w')\n",
    "            targets_h5.create_dataset('targets', data=y_true_hip)\n",
    "            targets_h5.create_dataset('genes', data=gene_ids)\n",
    "            targets_h5.close()\n",
    "    \n",
    "    # Save the weights\n",
    "    if celltype == 0:\n",
    "        weights = pd.DataFrame(np.transpose(weights), index=RBP.columns, columns=['Neurons'])\n",
    "        weights.to_csv(out_neurons + '/weights.csv')\n",
    "    else:\n",
    "        weights = pd.DataFrame(np.transpose(weights), index=RBP.columns, columns=['Glia'])\n",
    "        weights.to_csv(out_glia + '/weights.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06da08f",
   "metadata": {},
   "source": [
    "### Example on human HPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23518e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glia_dir = '../../Zenodo/Human/HPC/PSI/PSI_glia_norm.csv'\n",
    "neur_dir = '../../Zenodo/Human/HPC/PSI/PSI_neur_norm.csv'\n",
    "RBP_dir = '../../Zenodo/Human/HPC/RBP/RBP_peaks.csv'\n",
    "\n",
    "PSI_glia = pd.read_csv(glia_dir, index_col = 0)\n",
    "PSI_neur = pd.read_csv(neur_dir, index_col = 0)\n",
    "RBP = pd.read_csv(RBP_dir, index_col = 0)\n",
    "\n",
    "to_keep = PSI_glia['0'].notna() & PSI_neur['0'].notna()\n",
    "\n",
    "PSI_glia = PSI_glia[to_keep]\n",
    "PSI_neur = PSI_neur[to_keep]\n",
    "RBP = RBP[to_keep]\n",
    "\n",
    "lr = 0.5\n",
    "alpha = 0.001\n",
    "l1_ratio = 0.005\n",
    "batch_size = 256\n",
    "\n",
    "var_thres = 0.25 \n",
    "\n",
    "out_neurons = 'test_output/HPC/neurons/LR_var025/'\n",
    "out_glia = 'test_output/HPC/glia/LR_var01/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "681e8fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AATF_start</th>\n",
       "      <th>ABCF1_start</th>\n",
       "      <th>AKAP1_start</th>\n",
       "      <th>APOBEC3C_start</th>\n",
       "      <th>AQR_start</th>\n",
       "      <th>BCCIP_start</th>\n",
       "      <th>BUD13_start</th>\n",
       "      <th>CDC40_start</th>\n",
       "      <th>CPEB4_start</th>\n",
       "      <th>CPSF6_start</th>\n",
       "      <th>...</th>\n",
       "      <th>UTP18_end_overlap</th>\n",
       "      <th>UTP3_end_overlap</th>\n",
       "      <th>WDR3_end_overlap</th>\n",
       "      <th>WDR43_end_overlap</th>\n",
       "      <th>XPO5_end_overlap</th>\n",
       "      <th>YBX3_end_overlap</th>\n",
       "      <th>YWHAG_end_overlap</th>\n",
       "      <th>ZC3H11A_end_overlap</th>\n",
       "      <th>ZNF622_end_overlap</th>\n",
       "      <th>ZRANB2_end_overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chr10_100002942_100003023_ENSG00000107554_-</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr10_1000677_1000868_ENSG00000107937_+</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr10_100158984_100159055_ENSG00000107566_-</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr10_100167348_100167406_ENSG00000107566_-</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr10_100174208_100174281_ENSG00000107566_-</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrX_96936374_96936500_ENSG00000147202_+</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrX_96937233_96937351_ENSG00000147202_+</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrX_96970817_96970861_ENSG00000147202_+</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrX_9709633_9709760_ENSG00000101849_+</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrX_9741338_9741455_ENSG00000101850_-</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42942 rows × 732 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             AATF_start  ABCF1_start  \\\n",
       "chr10_100002942_100003023_ENSG00000107554_-         0.0          0.0   \n",
       "chr10_1000677_1000868_ENSG00000107937_+             1.0          0.0   \n",
       "chr10_100158984_100159055_ENSG00000107566_-         0.0          0.0   \n",
       "chr10_100167348_100167406_ENSG00000107566_-         0.0          0.0   \n",
       "chr10_100174208_100174281_ENSG00000107566_-         0.0          0.0   \n",
       "...                                                 ...          ...   \n",
       "chrX_96936374_96936500_ENSG00000147202_+            0.0          0.0   \n",
       "chrX_96937233_96937351_ENSG00000147202_+            0.0          0.0   \n",
       "chrX_96970817_96970861_ENSG00000147202_+            0.0          0.0   \n",
       "chrX_9709633_9709760_ENSG00000101849_+              0.0          0.0   \n",
       "chrX_9741338_9741455_ENSG00000101850_-              0.0          0.0   \n",
       "\n",
       "                                             AKAP1_start  APOBEC3C_start  \\\n",
       "chr10_100002942_100003023_ENSG00000107554_-          0.0             0.0   \n",
       "chr10_1000677_1000868_ENSG00000107937_+              0.0             0.0   \n",
       "chr10_100158984_100159055_ENSG00000107566_-          0.0             0.0   \n",
       "chr10_100167348_100167406_ENSG00000107566_-          0.0             0.0   \n",
       "chr10_100174208_100174281_ENSG00000107566_-          0.0             0.0   \n",
       "...                                                  ...             ...   \n",
       "chrX_96936374_96936500_ENSG00000147202_+             0.0             0.0   \n",
       "chrX_96937233_96937351_ENSG00000147202_+             0.0             0.0   \n",
       "chrX_96970817_96970861_ENSG00000147202_+             0.0             0.0   \n",
       "chrX_9709633_9709760_ENSG00000101849_+               0.0             0.0   \n",
       "chrX_9741338_9741455_ENSG00000101850_-               0.0             0.0   \n",
       "\n",
       "                                             AQR_start  BCCIP_start  \\\n",
       "chr10_100002942_100003023_ENSG00000107554_-        0.0          0.0   \n",
       "chr10_1000677_1000868_ENSG00000107937_+            0.0          0.0   \n",
       "chr10_100158984_100159055_ENSG00000107566_-        0.0          0.0   \n",
       "chr10_100167348_100167406_ENSG00000107566_-        0.0          0.0   \n",
       "chr10_100174208_100174281_ENSG00000107566_-        2.0          0.0   \n",
       "...                                                ...          ...   \n",
       "chrX_96936374_96936500_ENSG00000147202_+           0.0          0.0   \n",
       "chrX_96937233_96937351_ENSG00000147202_+           0.0          0.0   \n",
       "chrX_96970817_96970861_ENSG00000147202_+           0.0          0.0   \n",
       "chrX_9709633_9709760_ENSG00000101849_+             2.0          0.0   \n",
       "chrX_9741338_9741455_ENSG00000101850_-             0.0          0.0   \n",
       "\n",
       "                                             BUD13_start  CDC40_start  \\\n",
       "chr10_100002942_100003023_ENSG00000107554_-          1.0          0.0   \n",
       "chr10_1000677_1000868_ENSG00000107937_+              1.0          0.0   \n",
       "chr10_100158984_100159055_ENSG00000107566_-          0.0          0.0   \n",
       "chr10_100167348_100167406_ENSG00000107566_-          0.0          0.0   \n",
       "chr10_100174208_100174281_ENSG00000107566_-          0.0          1.0   \n",
       "...                                                  ...          ...   \n",
       "chrX_96936374_96936500_ENSG00000147202_+             0.0          0.0   \n",
       "chrX_96937233_96937351_ENSG00000147202_+             0.0          0.0   \n",
       "chrX_96970817_96970861_ENSG00000147202_+             0.0          0.0   \n",
       "chrX_9709633_9709760_ENSG00000101849_+               3.0          0.0   \n",
       "chrX_9741338_9741455_ENSG00000101850_-               0.0          0.0   \n",
       "\n",
       "                                             CPEB4_start  CPSF6_start  ...  \\\n",
       "chr10_100002942_100003023_ENSG00000107554_-          0.0          0.0  ...   \n",
       "chr10_1000677_1000868_ENSG00000107937_+              0.0          0.0  ...   \n",
       "chr10_100158984_100159055_ENSG00000107566_-          0.0          0.0  ...   \n",
       "chr10_100167348_100167406_ENSG00000107566_-          0.0          0.0  ...   \n",
       "chr10_100174208_100174281_ENSG00000107566_-          0.0          0.0  ...   \n",
       "...                                                  ...          ...  ...   \n",
       "chrX_96936374_96936500_ENSG00000147202_+             0.0          0.0  ...   \n",
       "chrX_96937233_96937351_ENSG00000147202_+             0.0          0.0  ...   \n",
       "chrX_96970817_96970861_ENSG00000147202_+             0.0          0.0  ...   \n",
       "chrX_9709633_9709760_ENSG00000101849_+               0.0          0.0  ...   \n",
       "chrX_9741338_9741455_ENSG00000101850_-               0.0          0.0  ...   \n",
       "\n",
       "                                             UTP18_end_overlap  \\\n",
       "chr10_100002942_100003023_ENSG00000107554_-                0.0   \n",
       "chr10_1000677_1000868_ENSG00000107937_+                    1.0   \n",
       "chr10_100158984_100159055_ENSG00000107566_-                0.0   \n",
       "chr10_100167348_100167406_ENSG00000107566_-                0.0   \n",
       "chr10_100174208_100174281_ENSG00000107566_-                0.0   \n",
       "...                                                        ...   \n",
       "chrX_96936374_96936500_ENSG00000147202_+                   0.0   \n",
       "chrX_96937233_96937351_ENSG00000147202_+                   0.0   \n",
       "chrX_96970817_96970861_ENSG00000147202_+                   0.0   \n",
       "chrX_9709633_9709760_ENSG00000101849_+                     0.0   \n",
       "chrX_9741338_9741455_ENSG00000101850_-                     0.0   \n",
       "\n",
       "                                             UTP3_end_overlap  \\\n",
       "chr10_100002942_100003023_ENSG00000107554_-               0.0   \n",
       "chr10_1000677_1000868_ENSG00000107937_+                   0.0   \n",
       "chr10_100158984_100159055_ENSG00000107566_-               0.0   \n",
       "chr10_100167348_100167406_ENSG00000107566_-               0.0   \n",
       "chr10_100174208_100174281_ENSG00000107566_-               0.0   \n",
       "...                                                       ...   \n",
       "chrX_96936374_96936500_ENSG00000147202_+                  0.0   \n",
       "chrX_96937233_96937351_ENSG00000147202_+                  0.0   \n",
       "chrX_96970817_96970861_ENSG00000147202_+                  0.0   \n",
       "chrX_9709633_9709760_ENSG00000101849_+                    0.0   \n",
       "chrX_9741338_9741455_ENSG00000101850_-                    0.0   \n",
       "\n",
       "                                             WDR3_end_overlap  \\\n",
       "chr10_100002942_100003023_ENSG00000107554_-               0.0   \n",
       "chr10_1000677_1000868_ENSG00000107937_+                   0.0   \n",
       "chr10_100158984_100159055_ENSG00000107566_-               0.0   \n",
       "chr10_100167348_100167406_ENSG00000107566_-               0.0   \n",
       "chr10_100174208_100174281_ENSG00000107566_-               0.0   \n",
       "...                                                       ...   \n",
       "chrX_96936374_96936500_ENSG00000147202_+                  0.0   \n",
       "chrX_96937233_96937351_ENSG00000147202_+                  0.0   \n",
       "chrX_96970817_96970861_ENSG00000147202_+                  0.0   \n",
       "chrX_9709633_9709760_ENSG00000101849_+                    0.0   \n",
       "chrX_9741338_9741455_ENSG00000101850_-                    0.0   \n",
       "\n",
       "                                             WDR43_end_overlap  \\\n",
       "chr10_100002942_100003023_ENSG00000107554_-                0.0   \n",
       "chr10_1000677_1000868_ENSG00000107937_+                    0.0   \n",
       "chr10_100158984_100159055_ENSG00000107566_-                0.0   \n",
       "chr10_100167348_100167406_ENSG00000107566_-                0.0   \n",
       "chr10_100174208_100174281_ENSG00000107566_-                0.0   \n",
       "...                                                        ...   \n",
       "chrX_96936374_96936500_ENSG00000147202_+                   0.0   \n",
       "chrX_96937233_96937351_ENSG00000147202_+                   0.0   \n",
       "chrX_96970817_96970861_ENSG00000147202_+                   0.0   \n",
       "chrX_9709633_9709760_ENSG00000101849_+                     0.0   \n",
       "chrX_9741338_9741455_ENSG00000101850_-                     0.0   \n",
       "\n",
       "                                             XPO5_end_overlap  \\\n",
       "chr10_100002942_100003023_ENSG00000107554_-               0.0   \n",
       "chr10_1000677_1000868_ENSG00000107937_+                   0.0   \n",
       "chr10_100158984_100159055_ENSG00000107566_-               0.0   \n",
       "chr10_100167348_100167406_ENSG00000107566_-               1.0   \n",
       "chr10_100174208_100174281_ENSG00000107566_-               1.0   \n",
       "...                                                       ...   \n",
       "chrX_96936374_96936500_ENSG00000147202_+                  0.0   \n",
       "chrX_96937233_96937351_ENSG00000147202_+                  0.0   \n",
       "chrX_96970817_96970861_ENSG00000147202_+                  0.0   \n",
       "chrX_9709633_9709760_ENSG00000101849_+                    0.0   \n",
       "chrX_9741338_9741455_ENSG00000101850_-                    0.0   \n",
       "\n",
       "                                             YBX3_end_overlap  \\\n",
       "chr10_100002942_100003023_ENSG00000107554_-               0.0   \n",
       "chr10_1000677_1000868_ENSG00000107937_+                   0.0   \n",
       "chr10_100158984_100159055_ENSG00000107566_-               0.0   \n",
       "chr10_100167348_100167406_ENSG00000107566_-               0.0   \n",
       "chr10_100174208_100174281_ENSG00000107566_-               0.0   \n",
       "...                                                       ...   \n",
       "chrX_96936374_96936500_ENSG00000147202_+                  0.0   \n",
       "chrX_96937233_96937351_ENSG00000147202_+                  0.0   \n",
       "chrX_96970817_96970861_ENSG00000147202_+                  0.0   \n",
       "chrX_9709633_9709760_ENSG00000101849_+                    0.0   \n",
       "chrX_9741338_9741455_ENSG00000101850_-                    0.0   \n",
       "\n",
       "                                             YWHAG_end_overlap  \\\n",
       "chr10_100002942_100003023_ENSG00000107554_-                0.0   \n",
       "chr10_1000677_1000868_ENSG00000107937_+                    0.0   \n",
       "chr10_100158984_100159055_ENSG00000107566_-                0.0   \n",
       "chr10_100167348_100167406_ENSG00000107566_-                0.0   \n",
       "chr10_100174208_100174281_ENSG00000107566_-                0.0   \n",
       "...                                                        ...   \n",
       "chrX_96936374_96936500_ENSG00000147202_+                   0.0   \n",
       "chrX_96937233_96937351_ENSG00000147202_+                   0.0   \n",
       "chrX_96970817_96970861_ENSG00000147202_+                   0.0   \n",
       "chrX_9709633_9709760_ENSG00000101849_+                     0.0   \n",
       "chrX_9741338_9741455_ENSG00000101850_-                     0.0   \n",
       "\n",
       "                                             ZC3H11A_end_overlap  \\\n",
       "chr10_100002942_100003023_ENSG00000107554_-                  0.0   \n",
       "chr10_1000677_1000868_ENSG00000107937_+                      0.0   \n",
       "chr10_100158984_100159055_ENSG00000107566_-                  0.0   \n",
       "chr10_100167348_100167406_ENSG00000107566_-                  0.0   \n",
       "chr10_100174208_100174281_ENSG00000107566_-                  0.0   \n",
       "...                                                          ...   \n",
       "chrX_96936374_96936500_ENSG00000147202_+                     0.0   \n",
       "chrX_96937233_96937351_ENSG00000147202_+                     0.0   \n",
       "chrX_96970817_96970861_ENSG00000147202_+                     0.0   \n",
       "chrX_9709633_9709760_ENSG00000101849_+                       0.0   \n",
       "chrX_9741338_9741455_ENSG00000101850_-                       0.0   \n",
       "\n",
       "                                             ZNF622_end_overlap  \\\n",
       "chr10_100002942_100003023_ENSG00000107554_-                 0.0   \n",
       "chr10_1000677_1000868_ENSG00000107937_+                     1.0   \n",
       "chr10_100158984_100159055_ENSG00000107566_-                 0.0   \n",
       "chr10_100167348_100167406_ENSG00000107566_-                 1.0   \n",
       "chr10_100174208_100174281_ENSG00000107566_-                 0.0   \n",
       "...                                                         ...   \n",
       "chrX_96936374_96936500_ENSG00000147202_+                    0.0   \n",
       "chrX_96937233_96937351_ENSG00000147202_+                    0.0   \n",
       "chrX_96970817_96970861_ENSG00000147202_+                    0.0   \n",
       "chrX_9709633_9709760_ENSG00000101849_+                      0.0   \n",
       "chrX_9741338_9741455_ENSG00000101850_-                      0.0   \n",
       "\n",
       "                                             ZRANB2_end_overlap  \n",
       "chr10_100002942_100003023_ENSG00000107554_-                 0.0  \n",
       "chr10_1000677_1000868_ENSG00000107937_+                     0.0  \n",
       "chr10_100158984_100159055_ENSG00000107566_-                 0.0  \n",
       "chr10_100167348_100167406_ENSG00000107566_-                 0.0  \n",
       "chr10_100174208_100174281_ENSG00000107566_-                 0.0  \n",
       "...                                                         ...  \n",
       "chrX_96936374_96936500_ENSG00000147202_+                    0.0  \n",
       "chrX_96937233_96937351_ENSG00000147202_+                    0.0  \n",
       "chrX_96970817_96970861_ENSG00000147202_+                    0.0  \n",
       "chrX_9709633_9709760_ENSG00000101849_+                      0.0  \n",
       "chrX_9741338_9741455_ENSG00000101850_-                      0.0  \n",
       "\n",
       "[42942 rows x 732 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd23af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1456\n",
      "177\n",
      "3737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type              | Params\n",
      "----------------------------------------------\n",
      "0 | loss_fn | BCEWithLogitsLoss | 0     \n",
      "1 | linear  | Linear            | 733   \n",
      "2 | sigmoid | Sigmoid           | 0     \n",
      "----------------------------------------------\n",
      "733       Trainable params\n",
      "0         Non-trainable params\n",
      "733       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7dc85dcf7e461da1ae4214c101faef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcmmichielsen\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# Run for neurons\n",
    "train_model_celltype(PSI_neur, PSI_glia, RBP,\n",
    "                   lr, alpha, l1_ratio, batch_size, var_thres,\n",
    "                  out_neurons, out_glia, 0)\n",
    "\n",
    "# Run for glia\n",
    "train_model_celltype(PSI_neur, PSI_glia, RBP,\n",
    "                   lr, alpha, l1_ratio, batch_size, var_thres,\n",
    "                  out_neurons, out_glia, 1, RBP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e835e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
